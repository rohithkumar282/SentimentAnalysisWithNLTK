{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23d55a0c",
   "metadata": {},
   "source": [
    "# Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb8aa6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from random import shuffle\n",
    "from statistics import mean\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e164c028",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Twitter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f26c72cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[31mFalse\u001b[0m It's true... :-( http//t.co/G3gV2f73Bh\n",
      "\n",
      "\n",
      "> \u001b[31mFalse\u001b[0m @Cheeky_rob123 @Rviver1979 just said I wouldn't vote Tory you melt! ðŸ˜‚\n",
      "\n",
      "\n",
      "> \u001b[31mFalse\u001b[0m @Channel4News @Jag_Paw_Jack well labours just lost Scotland u fuckin Tories in red ties what happened to the working class????????\n",
      "\n",
      "\n",
      "> \u001b[31mTrue\u001b[0m Off to the USA! Rosh and I on our travels again :) (@ Glasgow International Airport (GLA) - @gla_airport) https//t.co/FS71kc8FZe\n",
      "\n",
      "\n",
      "> \u001b[31mFalse\u001b[0m miliband the bawless wonder . http//t.co/3pyI73g0Rt\n",
      "\n",
      "\n",
      "> \u001b[31mFalse\u001b[0m junmyeon looks so d*ddy here :( LOOK https//t.co/xSRggfOijW\n",
      "\n",
      "\n",
      "> \u001b[31mTrue\u001b[0m RT @Nigel_Farage: I'm proud of #UKIP's health policy, which the public has voted as the most popular #AskNigelFarage http//t.co/qEeaFWexC6\n",
      "\n",
      "\n",
      "> \u001b[31mFalse\u001b[0m @OwenJones84 Let's also remember, Owen, that Ed Miliband doesn't believe the last Labour govt over-spent. #bbcqt\n",
      "\n",
      "\n",
      "> \u001b[31mFalse\u001b[0m RT @JASEMARKRUTTER: David Cameron says no more tax rises until 2020. Why don't we believe him? Let's take a look......... http//t.co/lVwUuâ€¦\n",
      "\n",
      "\n",
      "> \u001b[31mFalse\u001b[0m RT @IndyForTheGuy: \"Vote for me or I will go in such a huff. And while I sulk, the tories will get back in, &amp; then you'll be sorry!\" http:/â€¦\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# gather data for tweets and do one step preprocessing \n",
    "tweets = [t.replace(\"://\", \"//\") for t in nltk.corpus.twitter_samples.strings()]\n",
    "\n",
    "# function to return classified class\n",
    "def is_positive_twitter(tweet):\n",
    "    \"\"\"True if tweet has positive compound sentiment, False otherwise.\"\"\"\n",
    "    return sia.polarity_scores(tweet)[\"compound\"] > 0\n",
    "\n",
    "# shuffle dataset\n",
    "shuffle(tweets)\n",
    "for tweet in tweets[:10]:\n",
    "    print(\">\", \"\\033[31m\" + str(is_positive_twitter(tweet)) + \"\\033[0m\", tweet)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3339c7",
   "metadata": {},
   "source": [
    "# Sentiment Analysis on Movies Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78732ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64.05% correct\n"
     ]
    }
   ],
   "source": [
    "# gather data for movie reviews\n",
    "positive_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "negative_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "all_review_ids = positive_review_ids + negative_review_ids\n",
    "\n",
    "# function to return classified class\n",
    "def is_positive_movies(review_id):\n",
    "    \"\"\"True if the average of all sentence compound scores is positive.\"\"\"\n",
    "    text = nltk.corpus.movie_reviews.raw(review_id)\n",
    "    scores = [\n",
    "        sia.polarity_scores(sentence)[\"compound\"]\n",
    "        for sentence in nltk.sent_tokenize(text)\n",
    "    ]\n",
    "    return mean(scores) > 0\n",
    "\n",
    "# shuffle dataset\n",
    "shuffle(all_review_ids)\n",
    "correct = 0\n",
    "\n",
    "# calculate % of correct response from model\n",
    "for review_id in all_review_ids:\n",
    "    if is_positive_movies(review_id):\n",
    "        if review_id in positive_review_ids:\n",
    "            correct += 1\n",
    "    else:\n",
    "        if review_id in negative_review_ids:\n",
    "            correct += 1\n",
    "print(F\"{correct / len(all_review_ids):.2%} correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba793774",
   "metadata": {},
   "source": [
    "# Custom NLTK Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ea23f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unwanted stop words\n",
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "\n",
    "def skip_unwanted(pos_tuple):\n",
    "    word, tag = pos_tuple\n",
    "    if not word.isalpha() or word in unwanted:\n",
    "        return False\n",
    "    if tag.startswith(\"NN\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# Parts of Speech Tagging excluding Stopwords for Positive Class Words\n",
    "positive_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"pos\"]))\n",
    ")]\n",
    "\n",
    "# Parts of Speech Tagging excluding Stopwords for Negative Class Words\n",
    "negative_words = [word for word, tag in filter(\n",
    "    skip_unwanted,\n",
    "    nltk.pos_tag(nltk.corpus.movie_reviews.words(categories=[\"neg\"]))\n",
    ")]\n",
    "\n",
    "# frequency distribution and combine as a single set\n",
    "positive_fd = nltk.FreqDist(positive_words)\n",
    "negative_fd = nltk.FreqDist(negative_words)\n",
    "\n",
    "common_set = set(positive_fd).intersection(negative_fd)\n",
    "\n",
    "for word in common_set:\n",
    "    del positive_fd[word]\n",
    "    del negative_fd[word]\n",
    "\n",
    "top_100_positive = {word for word, count in positive_fd.most_common(100)}\n",
    "top_100_negative = {word for word, count in negative_fd.most_common(100)}\n",
    "\n",
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "unwanted.extend([w.lower() for w in nltk.corpus.names.words()])\n",
    "\n",
    "# positive and negative bigram finders\n",
    "positive_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
    "    w for w in nltk.corpus.movie_reviews.words(categories=[\"pos\"])\n",
    "    if w.isalpha() and w not in unwanted\n",
    "])\n",
    "negative_bigram_finder = nltk.collocations.BigramCollocationFinder.from_words([\n",
    "    w for w in nltk.corpus.movie_reviews.words(categories=[\"neg\"])\n",
    "    if w.isalpha() and w not in unwanted\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25710e7f",
   "metadata": {},
   "source": [
    "# Training and Using a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9e448a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    features = dict()\n",
    "    wordcount = 0\n",
    "    compound_scores = list()\n",
    "    positive_scores = list()\n",
    "\n",
    "    for sentence in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word.lower() in top_100_positive:\n",
    "                wordcount += 1\n",
    "        compound_scores.append(sia.polarity_scores(sentence)[\"compound\"])\n",
    "        positive_scores.append(sia.polarity_scores(sentence)[\"pos\"])\n",
    "\n",
    "    # Adding 1 to the final compound score to always have positive numbers\n",
    "    # since some classifiers you'll use later don't work with negative numbers.\n",
    "    features[\"mean_compound\"] = mean(compound_scores) + 1\n",
    "    features[\"mean_positive\"] = mean(positive_scores)\n",
    "    features[\"wordcount\"] = wordcount\n",
    "\n",
    "    return features\n",
    "\n",
    "# iterate over the tokens and compute compund score\n",
    "features = [\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"pos\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "]\n",
    "features.extend([\n",
    "    (extract_features(nltk.corpus.movie_reviews.raw(review)), \"neg\")\n",
    "    for review in nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22a1ac83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               wordcount = 4                 pos : neg    =      3.9 : 1.0\n",
      "               wordcount = 2                 pos : neg    =      3.8 : 1.0\n",
      "               wordcount = 0                 neg : pos    =      1.7 : 1.0\n",
      "               wordcount = 1                 pos : neg    =      1.7 : 1.0\n",
      "           mean_positive = 0.1245            pos : neg    =      1.0 : 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6626666666666666"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use 1/4 of the set for training\n",
    "train_count = len(features) // 4\n",
    "shuffle(features)\n",
    "classifier = nltk.NaiveBayesClassifier.train(features[:train_count])\n",
    "classifier.show_most_informative_features(10)\n",
    "\n",
    "nltk.classify.accuracy(classifier, features[train_count:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
